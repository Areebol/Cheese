<!--
 * @Descripttion: 
 * @version: 1.0
 * @Author: Areebol
 * @Date: 2023-06-08 17:59:16
-->

# [Yao Fu] 预训练，指令微调，对齐，专业化：论大语言模型能力的来源[bilibili](https://www.bilibili.com/video/BV1Qs4y1h7pn/?spm_id_from=333.337.search-card.all.click&vd_source=1e55c5426b48b37e901ff0f78992e33f)

**Pretraning, Instruction Tuning, Alignment, Specialization On the Source of Large Language Model Abilities**

# 四个阶段
  - pretraining 
  - intruction tuning
  - alignment
  - specialization
  
# Motivation
  - Scaling Law
  - Emergent abilities

# Scaling Law
模型size scaling up的过程中，模型的performance如何变化
  - scaling曲线
    - Log-linear
    - Phase change

## Log-linear
**其他条件保持不变，模型指数增长对应效果的线性增长**
  - 数据指数增长
  - 模型参数指数增长
  - fine-tuning的token的指数增长
  - instruction tuning对应的instruction的丰富程度对应跨领域的泛化能力

**对于建模的影响**
  - 在小模型上的改进，被大模型的参数带来的改进直接冲淡
  - 最快最有效的算法就是直接加大模型的参数
  - 在小模型上的结论可能在大模型上不成立
  - 有效的改进 <改进scaling law的曲线>
    - 改良的模型架构？
    - 改良的目标函数？
    - 数据集的污染？

## Emergent abilities
**模型参数量到达一定程度涌现新的能力**
小的模型无论如何努力,能力都被限制在了一定的区间
大的模型,参数到达一定量,就涌现出了很多新的能力

# LLM Model Families
- 看不同模型的演化过程
  - 从演化视角可以看清楚模型的动态变化
  - 每一步的变化与之前一步的关系
  - 从整体的架构上看大模型

**从模型家族的演化看大模型**
- Google PaLM
- DeepMind Gopher/Chinchilla
- OpenAI GPT

基本上经过三个阶段
- Pretraining(实现一个很强的base模型)
  - 用很大的模型在很大的数据集上做很久的训练,得到基本的能力
- Instruction Tuning(instruction数据训练提升一大截)
  - 对不同的任务给出不同任务描述指令
  - 指令越丰富,就可以泛化到更多没见过的指令
- Alignment(牺牲能力换安全)

<猜想>
在代码的数据集上进行训练可能得到链式思维

<instruction tunning>
更关键的是instruction的种类丰富程度,而不是instruction下的example数量
核心:
有的能力-更强
没有的能力-有

<Alignment>
Alignment tax - 对齐税
模型的输出符合人类的期望

- RLHF
  - 人类选择模型生成的数据
- Supervised learning
  - 人类直接去标注数据

# 模型专业化
给定基座模型,如何利用有限的算力,让模型在某个能力上拉满

# 结论
前面三个阶段都非常重要

由于scaling law的存在,小模型的能力被锁住,导致无法超越大模型

# question
- 如何构造干净的instruction 数据集非常重要
- instruction的数据集规模? [LM self-instruct]
