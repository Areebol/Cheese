<!--
 * @Descripttion: 
 * @version: 1.0
 * @Author: Areebol
 * @Date: 2023-07-01 10:24:54
-->

[paper](https://arxiv.org/pdf/2306.13549.pdf)
[github](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)
# authors
#TODO
# abstract
MLLM aka.(Multimodal Large Language Model) use powerful large language models as a brain to perform multimodal tasks.
**progress of MLLM**
- M-IT aka.(multimodal instruction tuning)
- M-ICL aka.(multimodal in-context learning)
- M-CoT aka.(multimodal chain of thought)
- LAVR aka.(LLM-Aided visual reasoning)

# introduction
NLP and Vision combination to generate MLLM

reasons for MLLM to overcome LLM
- MLLM is more in line with the way humans perceive the world
- MLLM offersuser-friendly interface
- MLLM is a more well-rounded task-solvers

this survey distinguish MLLM to four types
- M-IT
- M-ICL
- M-CoT
- LAVR

# overview
## M-IT
LLMs can be adapted for multimodality in terms of two aspects:
- architecture
- data
## M-ICL
an effective technique commonly used at the inference stage to boost few-shot performance

## M-CoT
used in complex resoning tasks
## LAVR
frequently involves the three techniques

# method
## Multimodal Instruction Tuning
### introduction
instruction tuning is a technique to improve performance on specific ares.

path
- pretrain finetune (BERT, T5)
  - require many task specific data to train a task-specific model
- prompting (GPT3)
  - reduce the reliance on large-scale data
  - fulfill a specialized task via prompting engineering
- instruction tuning (FLAN)
  - generalize to unseen tasks

transition from unimodal-instruction finetune to multimodal-instruction finetune
- data
  - acquire M-IT datasets by adapting existing benchmark datasets or by self-instruction
- model
  - inject the information of foreign modalities into LLMs and treat them as strong reasoners
  - align foreign embeddings to the LLMs 
  - translate foreign modalities into natural languages that LLMs can ingest


## Multimodal In-Context Learning
method 
- learn from a few examples along with an optional instruction and extrapolate to new questions
- implemeted in a training-free manner and thus can be flexibly integrated into different frameworks at the inference stages


## Multimodal Chain of Thought
### Modality bridging
through the fusion of features or through transforming visual input into textual descriptions

**learnable interface**
adopting a learnable interface to map visual embedding to the word embedding space

**expert model**
introduce an expert model to translate visual input to textual descriptions is an alternative modality bredging way

## LLM-Aided Visual Reasoning
taking LLMs as helpers with different roles
- strong generalization abilities
- emergent abilities
- better interactivity and control

# challenges and future directions
- current MLLMs are still limited in perception capabilities,leading to wrong visual information acquistion.
- the reasoning chain of MLLMs may be fragile.
- the instruction-following ability of MLLMs needs upgrading
- the object hallucination issue is widespread, affects the reliability of MLLLMs
- parameter-efficient training is needed.