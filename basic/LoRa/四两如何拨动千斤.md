<!--
 * @Descripttion: 
 * @version: 1.0
 * @Author: Areebol
 * @Date: 2023-06-10 18:45:17
-->

# [耿直LoRA](https://www.bilibili.com/video/BV1KV4y1m77E/?spm_id_from=444.41.list.card_archive.click&vd_source=ad232665e6f7da9a5121565507cf0816)

## 预备知识

### 矩阵的秩
**秩**
矩阵用作线性变换
- 旋转矩阵
$\left | \begin{matrix}
cos \theta & -sin\theta \\
sin \theta & cos \theta \\
\end{matrix}\right|$
变换后的图像二维，矩阵的秩2

- 矩阵2
$\left | \begin{matrix}
1 & -1 \\
1 & -1 \\
\end{matrix}\right|$
变换后的图像一维直线，矩阵的秩1

- 矩阵3
$\left | \begin{matrix}
0 & 0 \\
0 & 0
\end{matrix}\right|$
变换后的图像坍缩成0，矩阵的秩0

秩 matrix rank体现为列向量线性无关的个数，能够张成一个维度为 rank的空间

**矩阵分解**
一个矩阵分解成多个矩阵的乘积
- 奇异值分解 singular value decompostion SVD
- QR分解 
- LU分解
- 特征值分解

借助矩阵分解就可以快速处理高维数据和大规模数据

## 论文精讲
### LoRA: Low-Rank Adaptation of Large Language Model

认为模型微调的过程中只是低秩改变，虽然参数很多，但是真正微调的部分很少

优点
- 参数爆减
- 内存小
- 效果不变

$h = W_0 x + \Delta W x = W_0 x + BAx$

$W_0$ 被冻结

$\Delta W$ 被分解两个很小的矩阵 $A\ B$

**eg**

原来$\Delta W$是 $100*512$,现在直接降低到 $100*r +r*512$

初始将A用高斯缩放，B用0初始化

缺点
- 处理不同任务需要特殊处理

## 代码详解

**LoRa使用**
1. Installing `loralib` is simply
 ```
 pip install loralib
 # Alternatively
 # pip install git+https://github.com/microsoft/LoRA
 ```

 2. You can choose to adapt some layers by replacing them with counterparts implemented in `loralib`. We only support `nn.Linear`, `nn.Embedding`, and `nn.Conv2d` for now. We also support a `MergedLinear` for cases where a single `nn.Linear` represents more than one layers, such as in some implementations of the attention `qkv` projection (see Additional Notes for more).
 ```
 # ===== Before =====
 # layer = nn.Linear(in_features, out_features)

 # ===== After ======
 import loralib as lora
 # Add a pair of low-rank adaptation matrices with rank r=16
 layer = lora.Linear(in_features, out_features, r=16)
 ```

 3. Before the training loop begins, mark only LoRA parameters as trainable.
 ```
 import loralib as lora
 model = BigModel()
 # This sets requires_grad to False for all parameters without the string "lora_" in their names
 lora.mark_only_lora_as_trainable(model)
 # Training loop
 for batch in dataloader:
    ...
 ```
 4. When saving a checkpoint, generate a `state_dict` that only contains LoRA parameters.
 ```
 # ===== Before =====
 # torch.save(model.state_dict(), checkpoint_path)
 # ===== After =====
 torch.save(lora.lora_state_dict(model), checkpoint_path)
 ```
 5. When loading a checkpoint using `load_state_dict`, be sure to set `strict=False`.
 ```
 # Load the pretrained checkpoint first
 model.load_state_dict(torch.load('ckpt_pretrained.pt'), strict=False)
 # Then load the LoRA checkpoint
 model.load_state_dict(torch.load('ckpt_lora.pt'), strict=False)
 ```

 **loralib.utils**
 ```python
# set lora as trainable
def mark_only_lora_as_trainable(model: nn.Module, bias: str = 'none') -> None:
    # get n and p from model's parameters
    for n, p in model.named_parameters():
        # not lora, set grad -> false
        if 'lora_' not in n:
            p.requires_grad = False
    # no bias 
    if bias == 'none':
        return
    # exit bias, train all bias
    elif bias == 'all':
        # set grad 
        for n, p in model.named_parameters():
            if 'bias' in n:
                p.requires_grad = True
    # train only lora bias
    elif bias == 'lora_only':
        for m in model.modules():
            if isinstance(m, LoRALayer) and \
                hasattr(m, 'bias') and \
                m.bias is not None:
                    m.bias.requires_grad = True
    else:
        raise NotImplementedError
 ```
 
 **loralib.layers**
 ```python
#  LoRALayer -> basic module for other lora
class LoRALayer():
    # set basic attribute
    def __init__(
        self, 
        r: int, 
        lora_alpha: int, 
        lora_dropout: float,
        merge_weights: bool,
    ):
        # low rank 
        self.r = r
        # lora alpha
        self.lora_alpha = lora_alpha
        # Optional dropout
        if lora_dropout > 0.:
            self.lora_dropout = nn.Dropout(p=lora_dropout)
        else:
            self.lora_dropout = lambda x: x
        # Mark the weight as unmerged
        self.merged = False
        self.merge_weights = merge_weights

# rola Linear
class Linear(nn.Linear, LoRALayer):
    # LoRA implemented in a dense layer
    def __init__(
        self, 
        in_features: int, 
        out_features: int, 
        r: int = 0, 
        lora_alpha: int = 1, 
        lora_dropout: float = 0.,
        fan_in_fan_out: bool = False, # Set this to True if the layer to replace stores weight like (fan_in, fan_out)
        merge_weights: bool = True,
        **kwargs
    ):
        # use nn.linear and basic LoRALayer
        nn.Linear.__init__(self, in_features, out_features, **kwargs)
        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,
                           merge_weights=merge_weights)

        self.fan_in_fan_out = fan_in_fan_out
        # Actual trainable parameters
        if r > 0:
            # set loraA and loraB matrixs
            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))
            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))
            # set lora scaling
            self.scaling = self.lora_alpha / self.r
            # Freezing the pre-trained weight matrix
            self.weight.requires_grad = False
        # reset all parameters
        self.reset_parameters()
        if fan_in_fan_out:
            self.weight.data = self.weight.data.transpose(0, 1)

   # reset parameters
   def reset_parameters(self):
        # orig parameters reset
        nn.Linear.reset_parameters(self)
        # loraA use kaiming
        if hasattr(self, 'lora_A'):
            # initialize A the same way as the default for nn.Linear and B to zero
            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
            nn.init.zeros_(self.lora_B)

    # train 
    def train(self, mode: bool = True):
        # transpose w 
        def T(w):
            return w.transpose(0, 1) if self.fan_in_fan_out else w
        # train orig linear
        nn.Linear.train(self, mode)
        # do not merge weight
        if mode:
            if self.merge_weights and self.merged:
                # Make sure that the weights are not merged
                if self.r > 0:
                    self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling
                self.merged = False
        # merge weight
        else:
            if self.merge_weights and not self.merged:
                # Merge the weights and mark it
                if self.r > 0:
                    self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling
                self.merged = True       

    # forward 
    def forward(self, x: torch.Tensor):
        def T(w):
            return w.transpose(0, 1) if self.fan_in_fan_out else w
        # use lora and not merged
        if self.r > 0 and not self.merged:
            result = F.linear(x, T(self.weight), bias=self.bias)
            if self.r > 0:
                # result += lora
                result += (self.lora_dropout(x) @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling
            return result
        else:
            return F.linear(x, T(self.weight), bias=self.bias)
 ```
 